{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.contrib import predictor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import array\n",
    "from keras.models import model_from_json\n",
    "import urllib.request as request\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trieuchinh/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====start training>>>>>\n",
      "Step 0 _loss = 0.6863, Training Accuracy= 0.512\n",
      "Step 1 _loss = 0.6860, Training Accuracy= 0.512\n",
      "Step 500 _loss = 0.6415, Training Accuracy= 0.649\n",
      "Step 1000 _loss = 0.6035, Training Accuracy= 0.732\n",
      "Step 1500 _loss = 0.5803, Training Accuracy= 0.768\n",
      "Step 2000 _loss = 0.5670, Training Accuracy= 0.786\n",
      "Step 2500 _loss = 0.5575, Training Accuracy= 0.786\n",
      "Step 3000 _loss = 0.5487, Training Accuracy= 0.792\n",
      "Step 3500 _loss = 0.5398, Training Accuracy= 0.786\n",
      "Step 4000 _loss = 0.5303, Training Accuracy= 0.792\n",
      "Step 4500 _loss = 0.5214, Training Accuracy= 0.804\n",
      "Step 5000 _loss = 0.5144, Training Accuracy= 0.810\n",
      "Step 5500 _loss = 0.5089, Training Accuracy= 0.815\n",
      "Step 6000 _loss = 0.5043, Training Accuracy= 0.839\n",
      "Step 6500 _loss = 0.5002, Training Accuracy= 0.845\n",
      "Step 7000 _loss = 0.4966, Training Accuracy= 0.845\n",
      "Step 7500 _loss = 0.4935, Training Accuracy= 0.851\n",
      "Step 8000 _loss = 0.4906, Training Accuracy= 0.851\n",
      "Step 8500 _loss = 0.4881, Training Accuracy= 0.851\n",
      "Step 9000 _loss = 0.4859, Training Accuracy= 0.851\n",
      "Step 9500 _loss = 0.4838, Training Accuracy= 0.851\n",
      "Step 10000 _loss = 0.4819, Training Accuracy= 0.851\n",
      "Step 10500 _loss = 0.4801, Training Accuracy= 0.851\n",
      "Step 11000 _loss = 0.4783, Training Accuracy= 0.851\n",
      "Step 11500 _loss = 0.4766, Training Accuracy= 0.851\n",
      "Step 12000 _loss = 0.4750, Training Accuracy= 0.851\n",
      "Step 12500 _loss = 0.4734, Training Accuracy= 0.851\n",
      "Step 13000 _loss = 0.4718, Training Accuracy= 0.851\n",
      "Step 13500 _loss = 0.4703, Training Accuracy= 0.851\n",
      "Step 14000 _loss = 0.4688, Training Accuracy= 0.857\n",
      "Step 14500 _loss = 0.4673, Training Accuracy= 0.863\n",
      "Step 15000 _loss = 0.4659, Training Accuracy= 0.869\n",
      "Step 15500 _loss = 0.4646, Training Accuracy= 0.875\n",
      "Step 16000 _loss = 0.4632, Training Accuracy= 0.881\n",
      "Step 16500 _loss = 0.4619, Training Accuracy= 0.881\n",
      "Step 17000 _loss = 0.4605, Training Accuracy= 0.881\n",
      "Step 17500 _loss = 0.4592, Training Accuracy= 0.881\n",
      "Step 18000 _loss = 0.4579, Training Accuracy= 0.881\n",
      "Step 18500 _loss = 0.4566, Training Accuracy= 0.881\n",
      "Step 19000 _loss = 0.4554, Training Accuracy= 0.881\n",
      "Step 19500 _loss = 0.4541, Training Accuracy= 0.881\n",
      "Step 20000 _loss = 0.4529, Training Accuracy= 0.887\n",
      "Step 20500 _loss = 0.4518, Training Accuracy= 0.893\n",
      "Step 21000 _loss = 0.4506, Training Accuracy= 0.893\n",
      "Step 21500 _loss = 0.4495, Training Accuracy= 0.893\n",
      "Step 22000 _loss = 0.4485, Training Accuracy= 0.893\n",
      "Step 22500 _loss = 0.4475, Training Accuracy= 0.893\n",
      "Step 23000 _loss = 0.4465, Training Accuracy= 0.893\n",
      "Step 23500 _loss = 0.4456, Training Accuracy= 0.893\n",
      "Step 24000 _loss = 0.4446, Training Accuracy= 0.893\n",
      "Step 24500 _loss = 0.4438, Training Accuracy= 0.893\n",
      "Step 25000 _loss = 0.4430, Training Accuracy= 0.893\n",
      "Step 25500 _loss = 0.4422, Training Accuracy= 0.893\n",
      "Step 26000 _loss = 0.4414, Training Accuracy= 0.893\n",
      "Step 26500 _loss = 0.4406, Training Accuracy= 0.893\n",
      "Step 27000 _loss = 0.4399, Training Accuracy= 0.893\n",
      "Step 27500 _loss = 0.4393, Training Accuracy= 0.893\n",
      "Step 28000 _loss = 0.4386, Training Accuracy= 0.893\n",
      "Step 28500 _loss = 0.4380, Training Accuracy= 0.893\n",
      "Step 29000 _loss = 0.4375, Training Accuracy= 0.893\n",
      "Step 29500 _loss = 0.4369, Training Accuracy= 0.893\n",
      "Step 30000 _loss = 0.4364, Training Accuracy= 0.893\n",
      "Step 30500 _loss = 0.4359, Training Accuracy= 0.893\n",
      "Step 31000 _loss = 0.4354, Training Accuracy= 0.893\n",
      "Step 31500 _loss = 0.4350, Training Accuracy= 0.893\n",
      "Step 32000 _loss = 0.4345, Training Accuracy= 0.893\n",
      "Step 32500 _loss = 0.4341, Training Accuracy= 0.893\n",
      "Step 33000 _loss = 0.4337, Training Accuracy= 0.893\n",
      "Step 33500 _loss = 0.4333, Training Accuracy= 0.893\n",
      "Step 34000 _loss = 0.4329, Training Accuracy= 0.893\n",
      "Step 34500 _loss = 0.4326, Training Accuracy= 0.893\n",
      "Step 35000 _loss = 0.4323, Training Accuracy= 0.893\n",
      "Step 35500 _loss = 0.4319, Training Accuracy= 0.893\n",
      "Step 36000 _loss = 0.4316, Training Accuracy= 0.893\n",
      "Step 36500 _loss = 0.4313, Training Accuracy= 0.893\n",
      "Step 37000 _loss = 0.4310, Training Accuracy= 0.893\n",
      "Step 37500 _loss = 0.4308, Training Accuracy= 0.893\n",
      "Step 38000 _loss = 0.4305, Training Accuracy= 0.893\n",
      "Step 38500 _loss = 0.4302, Training Accuracy= 0.893\n",
      "Step 39000 _loss = 0.4300, Training Accuracy= 0.893\n",
      "Step 39500 _loss = 0.4298, Training Accuracy= 0.893\n",
      "Step 40000 _loss = 0.4295, Training Accuracy= 0.893\n",
      "Step 40500 _loss = 0.4293, Training Accuracy= 0.893\n",
      "Step 41000 _loss = 0.4291, Training Accuracy= 0.893\n",
      "Step 41500 _loss = 0.4289, Training Accuracy= 0.893\n",
      "Step 42000 _loss = 0.4287, Training Accuracy= 0.893\n",
      "Step 42500 _loss = 0.4285, Training Accuracy= 0.893\n",
      "Step 43000 _loss = 0.4284, Training Accuracy= 0.893\n",
      "Step 43500 _loss = 0.4282, Training Accuracy= 0.893\n",
      "Step 44000 _loss = 0.4280, Training Accuracy= 0.893\n",
      "Step 44500 _loss = 0.4279, Training Accuracy= 0.893\n",
      "Step 45000 _loss = 0.4277, Training Accuracy= 0.893\n",
      "Step 45500 _loss = 0.4276, Training Accuracy= 0.893\n",
      "Step 46000 _loss = 0.4274, Training Accuracy= 0.893\n",
      "Step 46500 _loss = 0.4273, Training Accuracy= 0.893\n",
      "Step 47000 _loss = 0.4271, Training Accuracy= 0.893\n",
      "Step 47500 _loss = 0.4270, Training Accuracy= 0.893\n",
      "Step 48000 _loss = 0.4269, Training Accuracy= 0.893\n",
      "Step 48500 _loss = 0.4268, Training Accuracy= 0.893\n",
      "Step 49000 _loss = 0.4266, Training Accuracy= 0.893\n",
      "Step 49500 _loss = 0.4265, Training Accuracy= 0.893\n",
      "[0, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_placeholders():\n",
    "    X = tf.placeholder(shape=(None, 11), dtype=tf.float64, name=\"X\")\n",
    "    Y = tf.placeholder(shape=(None, 2), dtype=tf.float64, name=\"Y\")\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def initialize_parameters(hidden_nodes):\n",
    "    W1 = tf.get_variable(\"W1\", [11, hidden_nodes], dtype=tf.float64,\n",
    "                         initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b1 = tf.get_variable(\"b1\", [hidden_nodes], dtype=tf.float64, initializer=tf.zeros_initializer())\n",
    "\n",
    "    W2 = tf.get_variable(\"W2\", [hidden_nodes, 2], dtype=tf.float64,\n",
    "                         initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b2 = tf.get_variable(\"b2\", [2], dtype=tf.float64, initializer=tf.zeros_initializer())\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2\n",
    "                  }\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    # Retrieve the parameters from the dictionary \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    # Build computational graph\n",
    "    Z1 = tf.add(tf.matmul(X, W1), b1)\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(A1, W2), b2)\n",
    "\n",
    "    y_hat = tf.sigmoid(Z2)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def compute_cost(y_hat, Y):\n",
    "    #     logits = tf.transpose(Z1)\n",
    "    #     labels = tf.transpose(Y)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_hat, labels=Y))\n",
    "    return cost\n",
    "\n",
    "def train_model(data_file):\n",
    "    csv_file = data_file\n",
    "    feature_names = [\"GCS\", \"GSS\", \"Permeability\", \"Moisture\", \"C\", \"Mn\", \"Si\", \"S\", \"P\", \"Cr\", \"Metal-Temp\"]\n",
    "    classes = [\"Scab\", \"Crack\", \"Blowhole\", \"Air-lock\", \"Misrun\", \"Defect\"]\n",
    "    # classes = [\"Scab\", \"Crack\", \"Defect\"]\n",
    "\n",
    "    train = pd.read_csv(csv_file)\n",
    "    Xtrain = train[feature_names].copy()\n",
    "    # normalize input feature using MinMax\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(Xtrain)\n",
    "    Xtrain_nor = pd.DataFrame(x_scaled)\n",
    "    # save scaling parameters into disk\n",
    "    joblib.dump(min_max_scaler, 'model/scaler.pkl')\n",
    "\n",
    "    print(\"====start training>>>>>\")\n",
    "    for itor in classes:\n",
    "        #start training model\n",
    "       \n",
    "        if itor == \"Defect\":\n",
    "            Ytrain = pd.get_dummies(train[itor])\n",
    "            model_nn(Xtrain_nor, Ytrain, label=itor)\n",
    "        else:\n",
    "            Ytrain = train[itor]\n",
    "            model_randomforest(Xtrain, Ytrain, itor)\n",
    "\n",
    "def model_randomforest(X_train, Y_train, label):\n",
    "    rfc = RandomForestClassifier(n_estimators=10).fit(X_train, Y_train)\n",
    "    model_file = \"model/\"+ label\n",
    "    joblib.dump(rfc, model_file)\n",
    "\n",
    "def model_nn(X_train, Y_train, label=\"Defect\", hidden_nodes=50, learning_rate=0.05,\n",
    "          momentum=0.09, num_iters=50000, show_log=True):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(1)\n",
    "    X, Y = create_placeholders()\n",
    "\n",
    "    W1 = tf.get_variable(\"W1\", [11, hidden_nodes], dtype=tf.float64,\n",
    "                         initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b1 = tf.get_variable(\"b1\", [hidden_nodes], dtype=tf.float64, initializer=tf.zeros_initializer())\n",
    "\n",
    "    W2 = tf.get_variable(\"W2\", [hidden_nodes, 2], dtype=tf.float64,\n",
    "                         initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b2 = tf.get_variable(\"b2\", [2], dtype=tf.float64, initializer=tf.zeros_initializer())\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2\n",
    "                  }\n",
    "\n",
    "    Y_hat = forward_propagation(X, parameters)\n",
    "    outputs = {'output': Y_hat}\n",
    "\n",
    "    cost = compute_cost(Y_hat, Y)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    correct_pred = tf.equal(tf.round(Y_hat), Y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    display_step = 500\n",
    "    saver = tf.train.Saver()\n",
    "    # Go through num_iters iterations\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        _, loss, acc =  sess.run([optimizer, cost, accuracy], feed_dict={X: X_train.values, Y: Y_train.values})\n",
    "        if i % display_step == 0 or i == 1:\n",
    "            print(\"Step \" + str(i) + \" _loss = {:.4f}\".format(loss)\n",
    "                  + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "        theta1 = sess.run(W1)\n",
    "        bias_1 = sess.run(b1)\n",
    "        theta2 = sess.run(W2)\n",
    "        bias_2 = sess.run(b2)\n",
    "    export_dir = \"model/\" + label\n",
    "    save_path = saver.save(sess, export_dir)\n",
    "#     print(\"Model saved in path: %s\" % save_path)\n",
    "    sess.close()\n",
    "    return True\n",
    "\n",
    "def Predict(arg1, arg2, arg3, arg4, arg5, arg6, arg7, arg8, arg9, arg10, arg11):\n",
    "    X_predict = array([[arg1, arg2, arg3, arg4, arg5, arg6, arg7, arg8, arg9, arg10, arg11]])\n",
    "    # load scaler to normalize data\n",
    "    min_max_scaler = joblib.load('model/scaler.pkl')\n",
    "    X_predict_nor = min_max_scaler.transform(X_predict)\n",
    "\n",
    "    classes = [\"Scab\", \"Crack\", \"Blowhole\", \"Air-lock\", \"Misrun\", \"Defect\"]\n",
    "    # classes = [\"Scab\", \"Crack\", \"Defect\"]\n",
    "    pre_arr = []\n",
    "    for itor in classes:\n",
    "        if itor == \"Defect\":\n",
    "            ret = predict_nn(X_predict_nor, itor)\n",
    "            pre_arr.append(ret)\n",
    "        else:\n",
    "            ret = predict_randomforest(X_predict, itor)\n",
    "            pre_arr.append(ret[0])\n",
    "            \n",
    "\n",
    "    return pre_arr\n",
    "\n",
    "def predict_randomforest(X_predict, label):\n",
    "    model_file_path = \"model/\" + label\n",
    "    loaded_model = joblib.load(model_file_path)\n",
    "    pred_val = loaded_model.predict(X_predict)\n",
    "    return pred_val\n",
    "    \n",
    "def predict_nn(X_predict, label, hidden_nodes=50):\n",
    "    tf.reset_default_graph()\n",
    "    W1 = tf.get_variable(\"W1\", [11, hidden_nodes], dtype=tf.float64)\n",
    "    b1 = tf.get_variable(\"b1\", [hidden_nodes], dtype=tf.float64)\n",
    "\n",
    "    W2 = tf.get_variable(\"W2\", [hidden_nodes, 2], dtype=tf.float64)\n",
    "    b2 = tf.get_variable(\"b2\", [2], dtype=tf.float64)\n",
    "\n",
    "    X = tf.placeholder(shape=(None, 11), dtype=tf.float64)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        # Restore variables from disk.\n",
    "        model_path = \"model/\"+ label\n",
    "        saver.restore(sess, model_path)\n",
    "        parameters = {\"W1\": W1,\n",
    "                      \"b1\": b1,\n",
    "                      \"W2\": W2,\n",
    "                      \"b2\": b2\n",
    "                      }\n",
    "\n",
    "        y_pred = forward_propagation(X, parameters)\n",
    "        prediction = sess.run(y_pred, feed_dict={X: X_predict})\n",
    "\n",
    "        if (prediction[0][0] > prediction[0][1]):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists('model'):\n",
    "        os.makedirs('model')\n",
    "    train_model(\"input_overall.csv\")\n",
    "    predict = Predict(348880, 130340,\t170,\t4.5,\t0.29,\t0.92,\t0.42,\t0.041,\t0.038,\t0.017,\t1610)\n",
    "\n",
    "    print(predict)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
